# ğŸš• NYC FHVHV Lakehouse Pipeline

Pipeline de datos escalable desarrollado para el anÃ¡lisis de viajes de vehÃ­culos de alquiler (Uber/Lyft) en NYC. Implementa una arquitectura **Lakehouse** de 3 capas sobre AWS Databricks.

## ğŸ—ï¸ Arquitectura

- **Capa Bronze**: Ingesta de archivos Parquet crudos con metadatos tÃ©cnicos.
- **Capa Silver**: Limpieza, validaciÃ³n de calidad y optimizaciÃ³n fÃ­sica mediante **Z-Order**.
- **Capa Gold**: AgregaciÃ³n de KPIs de negocio (Ingresos totales, volumen de viajes).

## ğŸš€ CÃ³mo ejecutar en Databricks

Para facilitar la entrega, he preparado scripts de importaciÃ³n directa que no requieren configuraciÃ³n manual de archivos:

1. Importa los notebooks desde las URLs de GitHub (ver guÃ­a adjunta).
2. AsegÃºrate de tener configurada la **External Location** en Unity Catalog para que el clÃºster pueda leer/escribir en S3.
3. Ejecuta los procesos en orden: `01_Bronze` -> `02_Silver` -> `03_Gold`.

## ğŸ› ï¸ TecnologÃ­as Usadas

- **PySpark**: Procesamiento distribuido.
- **Delta Lake**: Para transacciones ACID y optimizaciÃ³n de almacenamiento.
- **Unity Catalog**: Gobernanza y seguridad cloud-native.
- **AWS S3**: Almacenamiento persistente.

---
*Desarrollado por Sebastian Posada*
