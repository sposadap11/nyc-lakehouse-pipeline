# üöÄ Gu√≠a Maestra: Migraci√≥n Cloud-Native (AWS + Databricks)

> **Autor:** Sebastian Posada (`sebastian_posada`)  
> **Objetivo:** Ejecuci√≥n pipeline Lakehouse en Databricks Trial (AWS) con Unity Catalog y Z-Order Optimization.  
> **Costo Estimado:** < $1 USD (Usando Single Node & Auto-termination).

---

## üèóÔ∏è 1. Arquitectura Confirmada

- **Cloud:** AWS (S3 para Storage, IAM para Seguridad).
- **Compute:** Databricks en AWS (Versi√≥n Trial Premium pero optimizada).
- **Storage Access:** Unity Catalog (External Locations) -> **Seguridad de Grado Empresarial**.
- **Formato:** Delta Lake (Bronze/Silver/Gold).
- **Optimizaci√≥n:** Z-ORDER BY (`trip_date`) en capa Silver.

---

## üõ†Ô∏è 2. Preparaci√≥n del C√≥digo (Ya completado)

Tu archivo `src/common/config.py` ya fue actualizado para usar el protocolo `s3://` compatible con Unity Catalog.
Tus scripts ETL en `src/etl/` ya contienen la l√≥gica de Delta Lake y Z-Order.

---

## ‚ö° 3. Ejecuci√≥n en Databricks (Paso a Paso)

### Paso 1: Importar Repositorio (Nivel Senior)

En lugar de subir archivos sueltos, conectaremos tu Git.

1. Ve a tu carpeta local del proyecto en Windows.
2. Abre la terminal en esa carpeta y ejecuta:

   ```powershell
   git init
   git config user.name "sebastian_posada"
   git config user.email "sposadap11@gmail.com"
   git add .
   git commit -m "Initial commit: Cloud-Native Lakehouse on AWS Databricks"
   # Crea el repo en GitHub.com con el nombre 'nyc-lakehouse-pipeline'
   git remote add origin https://github.com/sposadap11/nyc-lakehouse-pipeline.git
   git push -u origin master
   ```

### Paso 2: Ejecutar en Databricks

1. En Databricks, ve a **Workspace** -> **Users** -> Tu usuario.

### Paso 2: Ejecutar en Databricks

1. En Databricks, ve a **Workspace** -> **Users** -> Tu usuario.
2. Clic derecho -> **Import** -> selecciona **URL**.
3. **Copia y pega estas URLs exactas (una por una):**

- **Libro 01_Bronze:**
    `https://raw.githubusercontent.com/sposadap11/nyc-lakehouse-pipeline/master/Databricks_01_Bronze.py`
- **Libro 02_Silver:**
    `https://raw.githubusercontent.com/sposadap11/nyc-lakehouse-pipeline/master/Databricks_02_Silver.py`
- **Libro 03_Gold:**
    `https://raw.githubusercontent.com/sposadap11/nyc-lakehouse-pipeline/master/Databricks_03_Gold.py`

*Nota: Databricks convertir√° autom√°ticamente estos archivos .py en notebooks funcionales.*

### Paso 3: Ejecuci√≥n Secuencial

1. **Ejecutar 01_Bronze:**
   - Lee de `s3://datalake-nyc-viajes-sebastian/raw/`
   - Escribe en `s3://datalake-nyc-viajes-sebastian/data/bronze` (Delta)
2. **Ejecutar 02_Silver (La magia ocurre aqu√≠):**
   - Lee Bronze.
   - Limpia y Transforma.
   - **Ejecuta OPTIMIZE ZORDER BY (trip_date)** -> *Esto es lo que buscar√°n en una entrevista t√©cnica.*
   - Escribe en `s3://.../data/silver`.
3. **Ejecutar 03_Gold:**
   - Calcula KPIs de Negocio (Revenue, Trips/Hour).
   - Escribe en `s3://.../data/gold`.

---

## üõ°Ô∏è 4. Estrategia de Defensa (Entrevista T√©cnica)

Cuando presentes esto, usa estos argumentos "Senior":

1. **¬øPor qu√© Databricks en AWS y no solo EMR/Glue?**
    - *"Eleg√≠ Databricks para aprovechar **Delta Lake** nativo y **Unity Catalog**. Esto me garantiza transacciones ACID y gobernanza de datos centralizada que Glue no ofrece "out of the box" con la misma facilidad."*

2. **¬øC√≥mo manejaste la optimizaci√≥n de costos?**
    - *"Configur√© el cl√∫ster como **Single Node** para evitar el overhead de workers innecesarios en un dataset mediano, y activ√© **Auto-termination (10 mins)** para asegurar zero-waste billing. El costo total fue centavos."*

3. **¬øQu√© optimizaci√≥n de rendimiento aplicaste?**
    - *"Implement√© **Z-ORDER Clustering** en la capa Silver basado en `trip_date`, ya que es el filtro m√°s com√∫n para los analistas. Esto reduce dr√°sticamente el I/O al hacer 'data skipping' en las consultas."*

4. **¬øPor qu√© Unity Catalog?**
    - *"Para evitar hardcodear Access Keys (`fs.s3a.*`) en el c√≥digo, lo cual es un riesgo de seguridad. Unity Catalog maneja la autenticaci√≥n v√≠a IAM Roles de forma transparente y auditable."*

---

## ‚úÖ Checklist Final

- [x] Configuraci√≥n AWS S3 + IAM
- [x] Databricks Unity Catalog Connection
- [x] C√≥digo Optimizado (Delta + Z-Order)
- [ ] Push a GitHub
- [ ] Ejecuci√≥n Exitosa
